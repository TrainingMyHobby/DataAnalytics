package tr.app.common.util.spark

//import org.apache.spark.SparkConf
//import org.apache.spark.SparkContext
//import org.apache.spark.sql.SparkSession

object SparkUtil {

  def initializeSparkContext(): Unit = {

    //    val spark = SparkSession.builder
    //      .appName("My Spark Application") // optional and will be autogenerated if not specified
    //      .master("local[*]") // only for demo and testing purposes, use spark-submit instead
    //      .enableHiveSupport() // self-explanatory, isn't it?
    //      .config("spark.sql.warehouse.dir", "target/spark-warehouse")
    //      .getOrCreate

  }

  //  def initializeSparkConf(): SparkConf = {
  //
  //    new SparkConf().setAppName("earthquake").setMaster("local[2]")
  //
  //  }

}